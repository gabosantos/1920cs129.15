{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2240ac55-1cc5-4e2d-94db-e070a5fc64e9",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "**Author**: Gabriel Lorenzo I. Santos (gsantos@ateneo.edu)\n",
    "\n",
    "----------\n",
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2023 Gabriel Lorenzo I. Santos\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "---------\n",
    "\n",
    "This notebook covers Supervised Learning techniques namely: K-Nearest Neigbors, Decision Trees, Naive-Bayes, and Regression. Here, we will use the __[scikit-learn](https://scikit-learn.org/stable/index.html)__ library to create and run the models.  As for our dataset, we will use the output of the previous __[Data Preprocessing](https://github.com/gabosantos/1920cs129.15/blob/master/2%20Titanic%20Data%20Preprocessing/script/Data%20Preprocessing%20in%20Python.ipynb)__ notebook. \n",
    "\n",
    "***Note: This notebook assumes that you are familiar with Python scripting and Jupyter notebook.  If you wish to learn the basics, check out the __[Data Preprocessing](https://github.com/gabosantos/1920cs129.15/blob/master/2%20Titanic%20Data%20Preprocessing/script/Data%20Preprocessing%20in%20Python.ipynb)__ notebook first.***\n",
    "\n",
    "Let's import pandas to persist our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badbac74-f8af-4507-99d9-5b5e4e08973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500dfe5",
   "metadata": {},
   "source": [
    "Then, we import our cleaned Titanic survivor dataset and save it to variable _df_.  Let's indicate that the _PassengerId_ column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a55edd3-617b-4c42-9e30-21c299bd810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/titanic_cleaned.csv', index_col=\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3da69",
   "metadata": {},
   "source": [
    "Let's view the top 5 rows of our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfd54ef-f150-47d9-849b-b467feece2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp  Parch      Fare  Sex_female  \\\n",
       "PassengerId                                                                   \n",
       "1                 0.0     1.0  0.375000    0.2    0.0  0.111538         0.0   \n",
       "3                 1.0     1.0  0.458333    0.0    0.0  0.121923         1.0   \n",
       "4                 1.0     0.0  0.645833    0.2    0.0  0.816923         1.0   \n",
       "5                 0.0     1.0  0.645833    0.0    0.0  0.123846         0.0   \n",
       "6                 0.0     1.0  0.535398    0.0    0.0  0.130128         0.0   \n",
       "\n",
       "             Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
       "PassengerId                                                \n",
       "1                 1.0         0.0         0.0         1.0  \n",
       "3                 0.0         0.0         0.0         1.0  \n",
       "4                 0.0         0.0         0.0         1.0  \n",
       "5                 1.0         0.0         0.0         1.0  \n",
       "6                 1.0         0.0         1.0         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e490f9-87ec-4a8a-9656-b48c73102bf8",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Before we begin with the model creation and usage, we need to split our dataset first to training set and test set.  The training set is used for model creation and the test set is used for model usage and evaluation. \n",
    "\n",
    "First, we need to separate the target/dependent variable from the features/independent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b28513e-3b18-4b0a-859f-01835f79f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Survived', axis=1)\n",
    "target = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfbf69",
   "metadata": {},
   "source": [
    "**drop()** is used to remove the column *Survived* from the original dataframe.  The resulting dataframe is then saved to the variable *features*.  \n",
    "\n",
    "The second line selects the *Survived* column alone from the original dataframe and is saved to variable *target*. Note that even if the *Survived* column is dropped first, we did not overwrite the original dataframe, allowing us to select the said column for the second line.\n",
    "\n",
    "We will now import our first Python module to help us split the dataset into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39e1678-e708-40ef-8b8b-6298a698ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04018d8",
   "metadata": {},
   "source": [
    "__[**train_test_split()**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)__ randomly selects rows from the features and target datasets and splits them into training and test datasets.  By default, the ratio of training and test datasets is 75:25.  You can change the split by adding the parameter _'test_size = X'_, where X is a number from 0-1, indicating the percentage of test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6b091c-ac57-4beb-a369-9bf54c568563",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e536b8",
   "metadata": {},
   "source": [
    "The output of the function is a 4-column list of the training and test datasets.  In the code above, they are assigned to *X_train*, *X_test*, *y_train*, and *y_test* variables, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07077b08",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "When testing the performance of a model, accuracy rate is one of the metrics being tracked.  **Accuracy Rate** refers to the % of correct guesses of the model (both true positives and true negatives) from all guesses made.  In order to get the true positives and true negatives, we will import and prepare our next Python module for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54bcafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a93bd-5c67-44e5-9a49-5c4739b3ddfe",
   "metadata": {},
   "source": [
    "------\n",
    "## K-Nearest Neighbors\n",
    "The K-Nearest Neighbors algorithm assigns an unlabeled or test datapoint with the label of the majority of its *K* neighbors. To use K-Nearest Neighbors, we will import the __[KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)__ from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10afe74c-b9e0-4fd8-9a3e-1e0372beaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520f3b0",
   "metadata": {},
   "source": [
    "Once imported, we will then create the KNeighborsClassifier object.  Here, we will input a very important parameter: *n_neighbors*, which dictates how many neighbors should the classifier look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5429dc-896a-4a04-aef6-038604695a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1346dea",
   "metadata": {},
   "source": [
    "In the code above, we defined our *k* to be 3, while the other parameters are left at their default values.  You can add more parameters if you wish to look at a different distance parameter, especially when you are looking at spatial data points when Euclidean distance might be impractical to use.\n",
    "\n",
    "The code below runs the first step in supervised learning: the **model creation**. As discussed in the lecture, it is usually done using the method *fit()* and it accepts two parameters: training features and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b14a40-b3fe-41a5-a4f3-5c7b66af0a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2d3a9",
   "metadata": {},
   "source": [
    "The second step in supervised learning is the **model usage**, where we will use the model created from the first step to start making guesses on our test set.  The main purpose of doing this is to test the accuracy of our model.  As discussed in the lecture, it is usually done using the method *predict()* and it accepts one parameter: test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d151baf9-e603-47e8-ac6d-5a651c44369c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4992938",
   "metadata": {},
   "source": [
    "The output of the *predict()* method is the predicted classification of the data points in our test set. This is what we are using to evaluate the model in __[*confusion_matrix()*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)__ method.  We include two parameters: the test labels and the predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f72bd73b-7aab-4602-8a65-e0c8bb399422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,  13],\n",
       "       [ 23,  35]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3ab9b",
   "metadata": {},
   "source": [
    "The output of the confusion matrix for binary classification is a 2x2 array indicating the true positives, false positives, false negatives, and true negatives.  We can unpack the array into these 4 individual variables to calculate the accuracy rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf980007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, knn.predict(X_test)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa83ce",
   "metadata": {},
   "source": [
    "For us to easily calculate the accuracy rate for the next algorithms, we will create our own function which accepts a confusion matrix as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b199bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7966101694915254"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_rate(confusion_matrix):\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "    return (tp + tn) / (tp + fp + fn + tn)\n",
    "\n",
    "accuracy_rate(confusion_matrix(y_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599cd307",
   "metadata": {},
   "source": [
    "This tells us that our K-Nearest Neighbors classifier is 79.66% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a0d7b-2747-497e-9dfe-ec972e0239e8",
   "metadata": {},
   "source": [
    "-----\n",
    "## Decision Trees\n",
    "The Decision Tree algorithm creates a tree of rules as to how each data point is classified based on the its features. To use decision trees, we will import the __[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)__ from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5608bd1-04b5-4255-81d6-3d090d234437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fe892",
   "metadata": {},
   "source": [
    "Similar to the previous classifier, we will create a DecisionTreeClassifier object with a parameter *random_state=0* to control the randomness of the splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63fbb9b5-100b-442e-9324-c1a37048292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776d764",
   "metadata": {},
   "source": [
    "We will proceed with the model creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef301940-ea0c-49ad-bed2-43a180b6567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f067cc",
   "metadata": {},
   "source": [
    "Then, we will then proceed with using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd56f302-fa39-4437-8598-8542d4ebd73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051e93e",
   "metadata": {},
   "source": [
    "Let's measure the accuracy of our decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23f745e6-0227-4af1-a0c6-1428c3c9ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7570621468926554"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_rate(confusion_matrix(y_test, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e4c2c",
   "metadata": {},
   "source": [
    "This shows that our decision tree classifier is 75.71% accurate, a bit lower than the accuracy of our K-Nearest Neighbor classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c28269-a3a4-47fc-9074-87b29a6eb353",
   "metadata": {},
   "source": [
    "-----\n",
    "## Naive-Bayes Classifier\n",
    "The Naive-Bayes Classifier assumes that each feature is conditionally independent from the other features, and all features are equally contributing to the outcome. For example, if we want to get the probability that a data point belongs to class $y$, given features/evidences $x_1, x_2, \\dots, x_n$, we calculate it using: $$P\\left(y|x_1, x_2, \\dots, x_n\\right) = \\dfrac{P\\left(x_1, x_2, \\dots, x_n|y\\right)\\cdot P\\left(y\\right)}{P\\left(x_1, x_2, ..., x_n\\right)}$$\n",
    "\n",
    "To use a Naive-Bayes classifier, we will import the __[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)__ from scikit-learn. Note that using this library assumes that our predictors follow a Gaussian or normal distribution.  For other types of distribution, check the rest of the NB classifiers __[here](https://scikit-learn.org/stable/modules/naive_bayes.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745ef5e2-14e2-47dc-9406-fc32ec52a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28ad31",
   "metadata": {},
   "source": [
    "We will proceed with GaussianNB object creation, model creation, model usage, and model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "327415a9-fa28-4f54-a84e-f603352fef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135593220338984"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the classifier object\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Create the model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Use the model\n",
    "nb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rate(confusion_matrix(y_test, nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5970601",
   "metadata": {},
   "source": [
    "This shows that our Naive-Bayes classifier is 81.36% accurate, a little bit higher than our K-Neighbors classifier and much higher than our Decision Tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00037d26-6e70-471b-9a93-be86da9afa48",
   "metadata": {},
   "source": [
    "------\n",
    "## Logistic Regression\n",
    "Logistic Regression is a type of regression where instead of predicting an unbounded continuous variable, the probability of the data point to belong to a class, bounded from 0 to 1, is predicted. To use a Logistic Regressor classifier, we will import the __[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)__ from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a90129e-430b-4d4d-aee4-e49f768805f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e4c87",
   "metadata": {},
   "source": [
    "We then instantiate our classifier object, create our model, use it and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5613faad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8192090395480226"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "LR.predict(X_test)\n",
    "accuracy_rate(confusion_matrix(y_test, LR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa099b",
   "metadata": {},
   "source": [
    "This shows that among the classifiers used in this notebook, the Logistic Regressor classifier is the most accuracte classifier at 80.79%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0672e",
   "metadata": {},
   "source": [
    "----------\n",
    "# Lab # 2: Supervised Learning\n",
    "Your objective is to create a classifier with a better accuracy than the ones created in this notebook.  To improve the accuracy, the following can be done:\n",
    "- **Re-process your working dataset**\n",
    "    - Perform data preprocessing again and select with columns should be included.\n",
    "    - Remove highly-correlated features and leave the one with higher correlation with the target variable.\n",
    "    - Try using the original dataset with categorical variables and compare with a dataset with purely numeric figures\n",
    "    - Try using an unstandardized dataset and compare to a standardized one.\n",
    "- **Change the test size**: Try other ratios of training and test set such as 70:30, 80:20, and 90:10.\n",
    "- **Change the model parameters**: Learn the different parameters that you can configure for each classifier and explore other configurations.\n",
    "\n",
    "Remember: There is no 1 perfect model for everything. Try different combinations and configurations!\n",
    "\n",
    "**Deliverables**:\n",
    "1. Jupyter notebook containing your code, results, and insights. \n",
    "    - Use the markdown cells to write your explanation as to why you selected such parameter/model configuration/dataset format. \n",
    "2. Certificate of Authorship: DISCS students should know where to get this. If you can't find it, please send me an email.\n",
    "\n",
    "**Due Date**: 21 April 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afdf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
